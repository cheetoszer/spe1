{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance, CollectionConfig, HnswConfig, OptimizersConfig\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonction load & chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def load_text_files(directory: str):\n",
    "    \"\"\"\n",
    "    Charge tout le contenu des fichiers .txt dans un dossier.\n",
    "    \"\"\"\n",
    "    texts = {}\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
    "                texts[filename] = file.read()\n",
    "    return texts\n",
    "\n",
    "def chunk_text_by_headers(text: str):\n",
    "    \"\"\"\n",
    "    Découpe un texte en chunks basés sur les sections délimitées par \"#\".\n",
    "    \n",
    "    Chaque section démarre par un \"#\". Le texte à l'intérieur de chaque section est\n",
    "    ensuite regroupé en un seul chunk, avec le caractère \"#\" supprimé.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Le texte à découper.\n",
    "        \n",
    "    Returns:\n",
    "        List[str]: Liste de chunks cohérents.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    sections = text.split(\"#\")\n",
    "    \n",
    "    for section in sections:\n",
    "        if section.strip():\n",
    "            chunks.append(section)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def save_chunks(chunks, output_directory, filename_base):\n",
    "    \"\"\"\n",
    "    Sauvegarde les chunks dans des fichiers .txt séparés.\n",
    "    \n",
    "    Args:\n",
    "        chunks (list): Liste de chunks à sauvegarder.\n",
    "        output_directory (str): Dossier de destination des fichiers.\n",
    "        filename_base (str): Base du nom de fichier pour chaque chunk.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    \n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        output_path = os.path.join(output_directory, f\"{filename_base}_chunk_{idx + 1}.txt\")\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(chunk)\n",
    "\n",
    "def main_load_chunk(input_directory: str, output_directory: str):\n",
    "    \"\"\"\n",
    "    Charge les fichiers .txt, effectue le chunking et sauvegarde les chunks.\n",
    "    \n",
    "    Args:\n",
    "        input_directory (str): Dossier contenant les fichiers .txt à traiter.\n",
    "        output_directory (str): Dossier où sauvegarder les chunks générés.\n",
    "    \n",
    "    Returns:\n",
    "        List[List[str]]: Liste de listes, chaque sous-liste contient les chunks pour un document.\n",
    "    \"\"\"\n",
    "\n",
    "    texts = load_text_files(input_directory)\n",
    "\n",
    "    all_chunks = [] \n",
    "\n",
    "    for filename, text in texts.items():\n",
    "        print(f\"Chunking du fichier : {filename}\")\n",
    "        chunks = chunk_text_by_headers(text)\n",
    "        \n",
    "        all_chunks.append(chunks)\n",
    "        \n",
    "        filename_base = os.path.splitext(filename)[0]\n",
    "        save_chunks(chunks, output_directory, filename_base)\n",
    "    \n",
    "    return all_chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Charger et chunker les docs txt dans \"documents\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking du fichier : rules.txt\n",
      "Chunking du fichier : faq.txt\n"
     ]
    }
   ],
   "source": [
    "input_directory = \"documents\"  # Répertoire contenant les fichiers .txt\n",
    "output_directory = \"chunks\"    # Répertoire où sauvegarder les chunks\n",
    "all_chunks = main_load_chunk(input_directory, output_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Fonctionnement :\\nLe joueur se déplace sur une grille à l'aide de bouton (haut, bas, gauche, droite) et résoud des problèmes mathématiques. Le but est d'atteindre une cible tout en répondant correctement aux questions mathématiques.\\n\\n\",\n",
       " \"Mécaniques de jeu :\\nLe joueur commence au niveau 1. La grille de jeu s'agrandit au fur et à mesure des niveaux. Il y a différents types d'opérations mathématiques : addition, soustraction, multiplication, division, puissances, problèmes algébriques.\\n\\n\",\n",
       " \"Système de score :\\nLe joueur gagne des points en répondant correctement aux questions. Le temps de réponse est pris en compte, une jauge de progression est présente. Les statistiques suivantes sont suivies : Nombre de réponses correctes, nombre total de tentatives, temps de réponse moyen, précision par type d'opération.\\n\\n\",\n",
       " \"Progression :\\nLe jeu devient plus difficile au fur et à mesure que le niveau augmente. La difficulté augmente par : L'agrandissement de la grille, des opérations mathématiques plus complexes, des nombres plus grands dans les calculs.\\n\\n\",\n",
       " \"Fin de partie :\\nLe jeu se termine quand le joueur ne parvient pas à atteindre l'objectif. Un tableau de bord est disponible pour voir ses performances, les statistiques sont analysées pour donner des retours sur : La mémoire de travail, la vitesse de traitement, La reconnaissance des motifs, la flexibilité cognitive, la résolution de problèmes, le contrôle de l'attention.\"]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_chunks[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonction Database & Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_subject(chunk):\n",
    "    \"\"\"\n",
    "    Extrait le sujet d'un chunk en capturant le texte avant le premier \":\".\n",
    "\n",
    "    Cette fonction prend un texte brut (chunk) et identifie le texte situé avant \n",
    "    le premier caractère \":\" pour le définir comme le sujet principal du chunk. \n",
    "    Si aucun \":\" n'est trouvé, elle renvoie \"Sujet inconnu\".\n",
    "\n",
    "    Args:\n",
    "        chunk (str): Le texte brut du chunk.\n",
    "\n",
    "    Returns:\n",
    "        str: Le texte avant le premier \":\" ou \"Sujet inconnu\" si \":\" n'est pas présent.\n",
    "    \"\"\"\n",
    "    if \":\" in chunk:\n",
    "        return chunk.split(\":\", 1)[0].strip()\n",
    "    return \"Sujet inconnu\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_embedding_model(model_name=\"intfloat/multilingual-e5-large-instruct\"):\n",
    "    \"\"\"\n",
    "    Charge un modèle d'embeddings et son tokenizer depuis Hugging Face.\n",
    "\n",
    "    Cette fonction initialise un modèle de transformation (Transformer) \n",
    "    ainsi que son tokenizer à partir du hub Hugging Face. Le modèle est\n",
    "    automatiquement déplacé sur un GPU (CUDA) s'il est disponible.\n",
    "\n",
    "    Args:\n",
    "        model_name (str, optional): Le nom du modèle Hugging Face à charger.\n",
    "                                    Par défaut : \"intfloat/multilingual-e5-large-instruct\".\n",
    "\n",
    "    Returns:\n",
    "        tuple: Un tuple contenant le tokenizer, le modèle, et l'appareil (CPU ou GPU).\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = model.to(device)\n",
    "    \n",
    "    return tokenizer, model, device\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def vectorize_text(texts, tokenizer, model, device):\n",
    "    \"\"\"\n",
    "    Vectorise une liste de textes en utilisant un modèle d'embeddings.\n",
    "\n",
    "    Cette fonction utilise le modèle et le tokenizer fournis pour transformer \n",
    "    chaque texte en un vecteur d'embedding. Les vecteurs résultants sont calculés\n",
    "    en prenant la moyenne des représentations des tokens pour chaque texte.\n",
    "\n",
    "    Args:\n",
    "        texts (list of str): Une liste de textes à vectoriser.\n",
    "        tokenizer (AutoTokenizer): Le tokenizer associé au modèle.\n",
    "        model (AutoModel): Le modèle de transformation (Transformer) chargé.\n",
    "        device (str): L'appareil sur lequel exécuter les calculs (\"cuda\" ou \"cpu\").\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Un tableau numpy contenant les vecteurs d'embedding pour chaque texte.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(**inputs).last_hidden_state.mean(dim=1)\n",
    "    \n",
    "    return embeddings.cpu().numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_qdrant_collection(client, collection_name):\n",
    "    \"\"\"\n",
    "    Crée une collection dans Qdrant si elle n'existe pas déjà.\n",
    "\n",
    "    Cette fonction vérifie si une collection Qdrant portant le nom spécifié existe. \n",
    "    Si elle n'existe pas, elle est créée avec une configuration par défaut, \n",
    "    incluant les paramètres pour HNSW et les optimisations.\n",
    "\n",
    "    Args:\n",
    "        client (QdrantClient): Une instance de QdrantClient connectée à un serveur Qdrant.\n",
    "        collection_name (str): Le nom de la collection à créer.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client.get_collection(collection_name)\n",
    "        print(f\"La collection '{collection_name}' existe déjà.\")\n",
    "    except Exception:\n",
    "        print(f\"Création de la collection '{collection_name}'...\")\n",
    "        hnsw_config = HnswConfig(m=16, ef_construct=200, full_scan_threshold=200).model_dump()\n",
    "        optimizer_config = OptimizersConfig(\n",
    "            deleted_threshold=0.5,\n",
    "            vacuum_min_vector_number=10000,\n",
    "            default_segment_number=5,\n",
    "            flush_interval_sec=30\n",
    "        ).model_dump()\n",
    "        vector_params = VectorParams(size=1024, distance=Distance.COSINE)\n",
    "\n",
    "        client.create_collection(\n",
    "            collection_name=collection_name,\n",
    "            vectors_config=vector_params,\n",
    "            hnsw_config=hnsw_config,\n",
    "            optimizers_config=optimizer_config\n",
    "        )\n",
    "        print(f\"Collection '{collection_name}' créée.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def insert_data_to_qdrant(client, collection_name, embeddings, ids, metadata):\n",
    "    \"\"\"\n",
    "    Insère des données (embeddings, identifiants, métadonnées) dans une collection Qdrant.\n",
    "\n",
    "    Cette fonction associe des vecteurs d'embedding à des identifiants uniques et des \n",
    "    métadonnées, puis les insère dans une collection Qdrant existante.\n",
    "\n",
    "    Args:\n",
    "        client (QdrantClient): Une instance de QdrantClient connectée à un serveur Qdrant.\n",
    "        collection_name (str): Le nom de la collection dans laquelle insérer les données.\n",
    "        embeddings (numpy.ndarray): Un tableau numpy contenant les vecteurs d'embedding.\n",
    "        ids (list of int): Une liste d'identifiants uniques pour chaque vecteur.\n",
    "        metadata (list of dict): Une liste de dictionnaires contenant les métadonnées associées.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    client.upsert(\n",
    "        collection_name=collection_name,\n",
    "        points=[\n",
    "            {\"id\": id_, \"vector\": embedding, \"payload\": meta} \n",
    "            for id_, embedding, meta in zip(ids, embeddings, metadata)\n",
    "        ]\n",
    "    )\n",
    "    print(f\"Les données ont été insérées dans la collection '{collection_name}'.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main_db_embedding(all_chunks, collection_name):\n",
    "    \"\"\"\n",
    "    Vectorise les chunks et les insère dans une collection Qdrant avec des métadonnées.\n",
    "\n",
    "    Cette fonction effectue les étapes suivantes :\n",
    "    1. Charge le modèle d'embedding et le client Qdrant.\n",
    "    2. Crée la collection Qdrant si elle n'existe pas.\n",
    "    3. Génère des métadonnées pour chaque chunk, incluant :\n",
    "        - La date de mise à jour.\n",
    "        - La catégorie (\"règles\" ou \"foire au question\").\n",
    "        - Le sujet extrait du chunk.\n",
    "    4. Vectorise les chunks en utilisant le modèle d'embedding.\n",
    "    5. Insère les vecteurs et les métadonnées dans Qdrant.\n",
    "\n",
    "    Args:\n",
    "        all_chunks (list of list of str): Une liste contenant deux listes de chunks :\n",
    "            - La première liste correspond aux \"règles\".\n",
    "            - La seconde liste correspond à la \"foire aux questions\".\n",
    "        collection_name (str): Le nom de la collection Qdrant cible.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    tokenizer, model, device = load_embedding_model()\n",
    "    client = QdrantClient(url=\"http://localhost:6333\")\n",
    "    create_qdrant_collection(client, collection_name)\n",
    "\n",
    "    today_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    metadata = []\n",
    "\n",
    "    for idx, chunks in enumerate(all_chunks):\n",
    "        category = \"règles\" if idx == 0 else \"foire au question\"\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            # Extraire le sujet du chunk\n",
    "            subject = extract_subject(chunk)\n",
    "            \n",
    "            metadata.append({\n",
    "                \"mis à jour\": today_date,\n",
    "                \"catégorie\": category,\n",
    "                \"sujet\": subject\n",
    "            })\n",
    "\n",
    "    texts = [chunk for chunks in all_chunks for chunk in chunks]\n",
    "    embeddings = vectorize_text(texts, tokenizer, model, device)\n",
    "    ids = list(range(len(texts)))\n",
    "\n",
    "    insert_data_to_qdrant(client, collection_name, embeddings, ids, metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La collection 'GameRag' existe déjà.\n",
      "Les données ont été insérées dans la collection 'GameRag'.\n"
     ]
    }
   ],
   "source": [
    "collection_name = \"GameRag\"\n",
    "main_db_embedding(all_chunks, collection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
