# Installation et Lancement du Projet

Ce guide explique comment installer et ex√©cuter le projet sur Ubuntu.

## Pr√©requis

Avant de commencer, assurez-vous d'avoir :
- **Python 3.8+**
- **Git**
- **Docker** (pour Qdrant)
- **CUDA** (si utilisation GPU)

---

## 1. Cloner le projet

```bash
git clone https://github.com/cheetoszer/spe1.git
cd spe1
```

## 2. Installer les d√©pendances Python

Cr√©ez un environnement virtuel et installez les d√©pendances :

```bash
python3 -m venv .venv
source .venv/bin/activate
pip install --upgrade pip
pip install -r requirements.txt
```

---

## 3. Installer et Lancer Qdrant via Docker

T√©l√©chargez et ex√©cutez l'image Docker de Qdrant :

```bash
docker pull qdrant/qdrant
docker run -d --name qdrant_container -p 6333:6333 qdrant/qdrant
```

V√©rifiez que Qdrant tourne bien :

```bash
docker ps | grep qdrant
telnet localhost 6333
```

---

### Installation GPU avec CUDA :

1. Assurez-vous d'avoir install√© le toolkit CUDA depuis le site officiel de NVIDIA.
2. Clonez le d√©p√¥t LlamaCpp et compilez avec CUDA activ√© :

```bash
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp
cmake -B build -DGGML_CUDA=ON
cmake --build build --config Release
```

Si votre GPU n'est pas d√©tect√© automatiquement, ajoutez la Compute Capability manuellement (ex : RTX 3080 Ti) :

```bash
cmake -B build -DGGML_CUDA=ON -DCMAKE_CUDA_ARCHITECTURES="86"
cmake --build build --config Release
```

**Documentation compl√®te de LlamaCpp :** [https://github.com/ggerganov/llama.cpp](https://github.com/ggerganov/llama.cpp)

---

## 5. Initialiser la base de donn√©es vectorielle

Ex√©cutez toutes les cellules de `ingest.ipynb` pour transformer les documents en vecteurs et les stocker dans Qdrant.

---

## 6. Lancer l'application

Une fois la base Qdrant initialis√©e, lancez l'application :

```bash
python app.py
```

Votre application est maintenant accessible et op√©rationnelle ! üöÄ











